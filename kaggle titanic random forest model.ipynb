{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0,
 "cells": [
  {
   "cell_type": "code",
   "source": "#################################################\n#### import our desired libraries\n#################################################\n\n# pandas\nimport pandas as pd\nfrom pandas import Series,DataFrame\n\n# numpy, matplotlib, seaborn, sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\n\n# setup a style to view ipython notebook graphs\nsns.set_style('whitegrid')\n\n#### import the data\ntest   = pd.read_csv('../input/test.csv')\ntrain    = pd.read_csv('../input/train.csv')",
   "execution_count": 2,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#################################################\n#### feature engineering \n#################################################\n\n\n###train['Ticket_group'] = np.where(train['Ticket'].str.isdigit(), train['Ticket'].astype(str).str[0], train['Ticket'].str[:1])\ntrain['Ticket_length'] = train['Ticket'].apply(lambda x: len(x))\n###test['Ticket_group'] = np.where(test['Ticket'].str.isdigit(), test['Ticket'].astype(str).str[0], test['Ticket'].str[:1])\ntest['Ticket_length'] = test['Ticket'].apply(lambda x: len(x))\n\ntrain[\"NameLength\"] = train[\"Name\"].apply(lambda x: len(x))\ntest[\"NameLength\"] = test[\"Name\"].apply(lambda x: len(x))\n\n\n\n########## this counts the number of spaces in the Name column\nimport re\n\nat = re.compile(r\" \", re.I)\ndef count_spaces(string):\n    count = 0\n    for i in at.finditer(string):\n        count += 1\n    return count\n\ntrain[\"spaces_in_name\"] = train[\"Name\"].map(count_spaces)\ntest[\"spaces_in_name\"] = test[\"Name\"].map(count_spaces)\n\n\n#########################################################\n##### This function returns the title from a name\n#########################################################\n\ndef title(name):\n# Search for a title using a regular expression. Titles are made of capital and lowercase letters ending with a period.\n    find_title = re.search(' ([A-Za-z]+)\\.', name)\n# Extract and return the title If it exists. \n    if find_title:\n        return find_title.group(1)\n    return \"\"\n\ntrain[\"Title\"] = train[\"Name\"].apply(title)\ntest[\"Title\"] = test[\"Name\"].apply(title)\n\n##### making some edits for the very small title groups \ntrain['Title'] = train['Title'].replace(['Don', 'Capt', 'Major', 'Sir', 'Rev', 'Col'], 'Sir')\ntrain['Title'] = train['Title'].replace(['Dona', 'Lady', 'the Countess', 'Jonkheer', 'Mme', 'Mlle', 'Countess'], 'Lady')\ntrain['Title'] = train['Title'].replace(['Ms'], 'Miss')\n\ntest['Title'] = test['Title'].replace(['Don', 'Capt', 'Major', 'Sir', 'Rev', 'Col'], 'Sir')\ntest['Title'] = test['Title'].replace(['Dona', 'Lady', 'the Countess', 'Jonkheer', 'Mme', 'Mlle', 'Countess'], 'Lady')\ntest['Title'] = test['Title'].replace(['Ms'], 'Miss')\n\n\n\n#########################################################\n##### add some additional interesting vars\n#########################################################\n\ntrain['Cabin_first_ltr'] = np.where(train['Cabin'].isnull(), 'Null', 'Not Null')\n##train['Parch_grouped'] = np.where(train['Parch'] > 0, '1', '0')\ntrain['FamilySize'] = train['SibSp'] + train['Parch']\ntrain['withfamily'] = np.where(train['FamilySize'] > 0, 1, 0)\ntrain['Female'] = np.where(train['Sex'] == 'female', 1, 0)\n\ntrain['miss'] = np.where(train['Name'].str.contains(\"Miss. \"), 1, 0)\ntrain['mrs'] = np.where(train['Name'].str.contains(\"Mrs. \"), 1, 0)\n\n\n\n#########################################################\n##### Group up the ticket variable\n#########################################################\ntrain[\"Ticket_grp\"] = np.where(train['Ticket'].str.isdigit(), train['Ticket'].astype(str).str[0], train[\"Ticket\"].str.split(' ').str.get(0))\ntest[\"Ticket_grp\"] = np.where(test['Ticket'].str.isdigit(), test['Ticket'].astype(str).str[0], test[\"Ticket\"].str.split(' ').str.get(0))",
   "execution_count": 26,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "###########################################################\n#### bucket the continuous age variable into categories\n###########################################################\n\ntrain['Age_grouped'], bins = pd.qcut(train['Age'], 10, retbins = True)\ntest['Age_grouped'] = pd.cut(test[\"Age\"], bins=bins, include_lowest=True)\n\ntrain['Fare_grouped'], Fare_bins = pd.qcut(train['Fare'], 4, retbins = True)\ntest['Fare_grouped'] = pd.cut(test[\"Fare\"], bins=Fare_bins, include_lowest=True)\n\n\n###### note that some of the age values were missing and thus the age_grouped is missing too\n#train['Age_grouped'][train['Age'] == train['Age'].median()].unique()\n\ntrain['Age_grouped'] = np.where(train['Age_grouped'].isnull(), \"(25, 28]\", train['Age_grouped'])\ntest['Age_grouped'] = np.where(test['Age_grouped'].isnull(), \"(25, 28]\", test['Age_grouped'])\n\ntrain['Fare_grouped'] = np.where(train['Fare_grouped'].isnull(), \"(7.91, 14.454]\", train['Fare_grouped'])\ntest['Fare_grouped'] = np.where(test['Fare_grouped'].isnull(), \"(7.91, 14.454]\", test['Fare_grouped'])\n\n##train['Fare_grouped'][train['Fare'] == train['Fare'].median()].unique()",
   "execution_count": 5,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "### what's the distribution of age? \n#sns.violinplot(data=train['Age'])\n\n#sns.violinplot(data = train['Fare'])",
   "execution_count": 4,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## to run a random forest we need to make sure the dataset doens't contain any missing values.\n### does it contain missing values\nif train.isnull().values.any() == True:\n    print(\"there are some missing values\")\nelse: \n    print(\"there are no missing values\")",
   "execution_count": 13,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Make adjustments to the test dataset to match the train dataset\n\ntest['Cabin_first_ltr'] = np.where(test['Cabin'].isnull(), 'Null', 'Not Null')\ntest['FamilySize'] = test['SibSp'] + test['Parch']\ntest['withfamily'] = np.where(test['FamilySize'] > 0, 1, 0)\ntest['Female'] = np.where(test['Sex'] == 'female', 1, 0)\n\ntest['miss'] = np.where(test['Name'].str.contains(\"Miss. \"), 1, 0)\ntest['mrs'] = np.where(test['Name'].str.contains(\"Mrs. \"), 1, 0)",
   "execution_count": 14,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "##### this removes the missing values\nfrom sklearn.base import TransformerMixin\nclass DataFrameImputer(TransformerMixin):\n    def fit(self, X, y=None):\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].median() for c in X],\n            index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n    \n\n\n### this will transfer the categorical variables to floats for the algo\ndef do_treatment(df):\n    for col in df:\n        if df[col].dtype == np.dtype('O'):\n            df[col] = df[col].apply(lambda x : hash(str(x)))\n\n    ",
   "execution_count": 16,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\ntrain_imputed = DataFrameImputer().fit_transform(train)\ntest_imputed = DataFrameImputer().fit_transform(test)\n\ndo_treatment(train_imputed)\ndo_treatment(test_imputed)",
   "execution_count": 17,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "######## Creating the random forest model \n# Create the random forest object which will include all the parameters\n# for the fit\nforest = RandomForestClassifier(n_estimators = 200, max_features = 'sqrt',\n                             max_depth = None, verbose = 1, n_jobs = -1)\n\n# Fit the training data to the Survived labels and create the decision trees\n#train_independent_vars = train_imputed.drop(['Survived'], axis = 1)\ntrain_independent_vars = train_imputed[['Ticket_length', 'Title', 'NameLength', 'Pclass', 'Female', 'Age_grouped', 'Ticket_grp', 'Fare_grouped', 'Cabin_first_ltr', 'spaces_in_name']]\ntrain_independent_vars = train_independent_vars\n##, 'Embarked', 'withfamily'\ntrain_dependent_vars = train_imputed['Survived']\n\nforest = forest.fit(train_independent_vars, train_dependent_vars)\n\n# Take the same decision trees and run it on the test data\noutput = forest.predict(test_imputed[['Ticket_length', 'Title', 'NameLength', 'Pclass', 'Female', 'Age_grouped', 'Ticket_grp', 'Fare_grouped', 'Cabin_first_ltr', 'spaces_in_name']])\n\n### combine the passengerid with the prediction\noutput_df = pd.DataFrame(test_imputed.PassengerId).join(pd.DataFrame(output))\noutput_df.columns = ['PassengerId', 'Survived']\n#### create the final output dataframe\nfinal_output = DataFrame(columns=['PassengerId', 'Survived'])\nfinal_output = final_output.append(output_df[['PassengerId', 'Survived']])\n\n#### convert from string to ints \nfinal_output['PassengerId'] = final_output['PassengerId'].astype(int)\nfinal_output['Survived'] = final_output['Survived'].astype(int)\n\n#### convert to csv\nfinal_output.to_csv('output.csv', index = False, header = ['PassengerId', 'Survived'])",
   "execution_count": 63,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#\nimportances = forest.feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in indices:\n    print(train_independent_vars.columns[f], importances[f])",
   "execution_count": 64,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#",
   "execution_count": 1,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#",
   "execution_count": 1,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#",
   "execution_count": 1,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#",
   "execution_count": 1,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#",
   "execution_count": 1,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  }
 ]
}